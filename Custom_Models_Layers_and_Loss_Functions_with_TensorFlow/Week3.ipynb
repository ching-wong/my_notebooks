{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ching-wong/my_notebooks/blob/main/Custom_Models_Layers_and_Loss_Functions_with_TensorFlow/Week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Models, Layers, and Loss Functions with TensorFlow - Week 3\n",
        "\n",
        "How to define custom layers by using lambda layer or as a class."
      ],
      "metadata": {
        "id": "VqW8A7V0q51z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Custom Models, Layers, and Loss Functions with TensorFlow - Week 3](#scrollTo=VqW8A7V0q51z)\n",
        "\n",
        ">>[What is a layer?](#scrollTo=X-TN-tNnjAGw)\n",
        "\n",
        ">>[Lambda layer](#scrollTo=GpV1fDf9gXlP)\n",
        "\n",
        ">>[Custom layer](#scrollTo=FtTiW_dgkwGA)\n",
        "\n",
        ">>[Adding an activation function](#scrollTo=vr_-qh6-rYiY)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "blo5iFwQe974"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a layer?\n",
        "\n",
        "A class in a neural network that holds parameters which define its state and computation.\n",
        "\n",
        "* State (weights): Variables that make each layer unique. They can be trainable (adjusted during training) or non-trainable (used for other purposes).\n",
        "\n",
        "* Computation: Transforms inputs into outputs, known as the forward pass, passing results to the next layer.\n",
        "\n",
        "For example, $Y = w * X + b$ is the computation, while $w$ and $b$ are the weights."
      ],
      "metadata": {
        "id": "X-TN-tNnjAGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lambda layer\n",
        "\n",
        "The easiest way to define a custom layer is to use the lambda layer. Here is a simple example, which turns the outputs to their absolute values."
      ],
      "metadata": {
        "id": "GpV1fDf9gXlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Lambda, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "input = Input(shape=[100])\n",
        "x = Lambda(lambda x: tf.abs(x))(input)"
      ],
      "metadata": {
        "id": "SKSRfslveYj4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, one can have a custom relu. In the following code, x and x2 are the same, while z is obtained by a small tweak of the relu."
      ],
      "metadata": {
        "id": "w2BFxz7ugcs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def my_relu_0(x):\n",
        "  return K.maximum(0.0, x)\n",
        "\n",
        "def my_relu_1(x):\n",
        "  return K.maximum(0.1, x)\n",
        "\n",
        "x = Dense(32, activation=\"relu\")(input)\n",
        "\n",
        "y = Dense(32)(input)\n",
        "x2 = Lambda(my_relu_0)\n",
        "z = Lambda(my_relu_1)"
      ],
      "metadata": {
        "id": "ISS80lqFgsDq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom layer\n",
        "\n",
        "To define a custom layer as a class, we first need to inherit it from the class Layer.\n",
        "\n",
        "Inside the class, there should be at least 3 methods:\n",
        "\n",
        "* init, accepts parameters and sets up internal variables\n",
        "\n",
        "* build, runs when the instance is created, for specifying local input states and\n",
        "\n",
        "* call, performs the computation, is called during training to get the output.\n",
        "\n",
        "The following code is for the layer $Y = w * X + b$.\n"
      ],
      "metadata": {
        "id": "FtTiW_dgkwGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class SimpleDense(Layer):\n",
        "\n",
        "  def __init__(self, units = 32):\n",
        "    super(SimpleDense, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    w_init = tf.random_uniform_initializer() # uses normal distribution. There are a few other options from tf\n",
        "    self.w = self.add_weight(name=\"kernel\",\n",
        "                                 shape=(input_shape[-1], self.units),\n",
        "                                 initializer=w_init,\n",
        "                                 trainable=True)\n",
        "\n",
        "    b_init = tf.zeros_initializer()\n",
        "    self.b = self.add_weight(name=\"bias\",\n",
        "                                 shape=(self.units,),\n",
        "                                 initializer=b_init,\n",
        "                                 trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b"
      ],
      "metadata": {
        "id": "nCnhtqp5lLlD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use .variables to see the values of the weights (before or after training)."
      ],
      "metadata": {
        "id": "3SswfBUhqkoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dense = SimpleDense(1)\n",
        "x = tf.ones((1,1))\n",
        "y = my_dense(x)\n",
        "print(my_dense.variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqjcx4u_oZtr",
        "outputId": "ed329a24-41c4-4556-fff7-efd10d94e529"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Variable path=simple_dense/kernel, shape=(1, 1), dtype=float32, value=[[-0.03975159]]>, <Variable path=simple_dense/bias, shape=(1,), dtype=float32, value=[0.]>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding an activation function\n",
        "\n",
        "In the init method, we accept one more parameter (the activation). In the call method, we wrap the result by the activation function."
      ],
      "metadata": {
        "id": "vr_-qh6-rYiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.activations import get\n",
        "\n",
        "class SimpleDense(Layer):\n",
        "\n",
        "  def __init__(self, units = 32, activation = None):\n",
        "    super(SimpleDense, self).__init__()\n",
        "    self.units = units\n",
        "    self.activation = get(activation) # get activation\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    w_init = tf.random_uniform_initializer()\n",
        "    self.w = self.add_weight(name=\"kernel\",\n",
        "                                 shape=(input_shape[-1], self.units),\n",
        "                                 initializer=w_init,\n",
        "                                 trainable=True)\n",
        "\n",
        "    b_init = tf.zeros_initializer()\n",
        "    self.b = self.add_weight(name=\"bias\",\n",
        "                                 shape=(self.units,),\n",
        "                                 initializer=b_init,\n",
        "                                 trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.w) + self.b) # wrap it with the activation function"
      ],
      "metadata": {
        "id": "MEXxgHb7nLI7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dense = SimpleDense(1, activation = \"relu\")\n",
        "x = tf.ones((1,1))\n",
        "y = my_dense(x)\n",
        "print(my_dense.variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4APkaALsrct",
        "outputId": "67837b48-86a6-4941-e41d-0c49bb4a070d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Variable path=simple_dense_1/kernel, shape=(1, 1), dtype=float32, value=[[-0.00663128]]>, <Variable path=simple_dense_1/bias, shape=(1,), dtype=float32, value=[0.]>]\n"
          ]
        }
      ]
    }
  ]
}